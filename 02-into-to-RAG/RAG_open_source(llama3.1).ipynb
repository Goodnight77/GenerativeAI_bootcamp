{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/drive/1K5u1xyfHBG0NmB2OwLz0U-bQqwnwQkB3\n",
    "\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Gentle Introduction to RAG Applications\n",
    "\n",
    "This notebook creates a simple RAG (Retrieval-Augmented Generation) system to answer questions from a PDF document using an open-source model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_FILE = \"FAQ_GDG.pdf\"\n",
    "\n",
    "# We'll be using Llama 3.1 8B for this example.\n",
    "MODEL = \"llama3.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the PDF document\n",
    "\n",
    "starting by loading the PDF document and breaking it down into separate pages.\n",
    "\n",
    "<img src='images/documents.png' width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: pypdf in c:\\users\\msi\\miniconda3\\envs\\gn\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in c:\\users\\msi\\appdata\\roaming\\python\\python310\\site-packages (from pypdf) (4.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages: 12\n",
      "Length of a page: 2037\n",
      "Content of a page: W h y w a s m y a c c o u n t t u r n e d t o p r i v a t e ? \n",
      " If we r easonably belie v e content in y our pr o ﬁ le violates our content policy , y our account will be switched t o priv ate and the content in y our pr o ﬁ le will be deleted. Y ou won 't be able t o mak e y our account public again for at least 60 da ys. Google also r eser v es the right t o suspend or terminate y our access t o the ser vices or delete y our Google Account, as described in the T aking action in case of pr oblems section of the Google T erms of Ser vice. \n",
      "W h a t h a p p e n s w h e n I i n t e g r a t e m y p r o ﬁ l e w i t h a t h i r d - p a r t y a p p o r s e r v i c e ? \n",
      " If y ou authoriz e an application t o access y our Google De v eloper Pr ogr am pr o ﬁ le, that application will be able t o see y our pr o ﬁ le information, e v en if y ou ha v e not made y our pr o ﬁ le public. Learn mor e about how t o manage thir d-par ty apps and ser vices with access t o y our account. \n",
      "C a n I t r a n s f e r o r m e r g e m y a c c o u n t w i t h a n o t h e r p r o ﬁ l e ? \n",
      " Y ou cannot tr ansf er or mer ge y our pr o ﬁ le with another account. It' s encour aged t o use y our personal account (when appr opriate) for y our Google De v eloper Pr ogr am pr o ﬁ le t o ensur e y ou r etain all the badges and information. \n",
      "W h y c a n ' t I c r e a t e a p r o ﬁ l e w i t h m y G o o g l e W o r k s p a c e a c c o u n t ? \n",
      " The Google De v eloper Pr ogr am pr o ﬁ le suppor ts Google W orkspace account types; howe v er , if y ou ar e getting an err or y ou might need y our or ganization ' s administr at or t o enable access t o the Google De v elopers ser vice. \n",
      " F or mor e information, see T urn Google De v elopers on or off for users . \n",
      "W h e r e s h o u l d I ﬁ l e i s s u e s o r f e e d b a c k ? \n",
      " If y ou encounter any issues or want t o pr o vide f eedback on anything r elated t o y our Google De v eloper Pr ogr am pr o ﬁ le, click Send F eedback at the bott om of y our Google De v eloper Pr ogr am pr o ﬁ le page . \n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(PDF_FILE)\n",
    "pages = loader.load()\n",
    "\n",
    "print(f\"Number of pages: {len(pages)}\")\n",
    "print(f\"Length of a page: {len(pages[1].page_content)}\")\n",
    "print(\"Content of a page:\", pages[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the pages in chunks\n",
    "\n",
    "Pages are too long, so let's split pages into different chunks.\n",
    "\n",
    "<img src='images/splitter.png' width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 36\n",
      "Length of a chunk: 582\n",
      "Content of a chunk: H o w d o I e d i t m y p r o ﬁ l e ? \n",
      " Y ou can edit y our Google De v eloper Pr ogr am pr o ﬁ le b y going t o de v elopers.google.com/pr o ﬁ le/u/me . \n",
      "W h a t h a p p e n s i f I m a k e m y p r o ﬁ l e p u b l i c ? \n",
      " Making y our pr o ﬁ le public mak es it viewable b y any one online. This includes y our name, image, r ole, company or school, bio, badges y ou'v e r eceiv ed, stats, and y our social media links (including GitHub, GitLab, X, Link edIn, and Stack Ov er ﬂ ow). Y our pages sa v ed, pages r ated, and e v ents attended ar e not par t of y our public pr o ﬁ le.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=100)\n",
    "\n",
    "chunks = splitter.split_documents(pages)\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(f\"Length of a chunk: {len(chunks[1].page_content)}\")\n",
    "print(\"Content of a chunk:\", chunks[1].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing the chunks in a vector store\n",
    "\n",
    "We can now generate embeddings for every chunk and store them in a vector store.\n",
    "we will use FAISS: Facebook AI Similarity Search\n",
    "\n",
    "<img src='images/vectorstore.png' width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-community faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "print(faiss.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-huggingface\n",
    "%pip install -qU langchain-ollama\n",
    "%pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI\\miniconda3\\envs\\gn\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "C:\\Users\\MSI\\miniconda3\\envs\\gn\\lib\\site-packages\\beartype\\_util\\error\\utilerrwarn.py:67: BeartypeModuleUnimportableWarning: Ignoring module \"onnx\" importation exception:\n",
      "    ImportError: DLL load failed while importing onnx_cpp2py_export: A dynamic link library (DLL) initialization routine failed.\n",
      "  warn(message, cls)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "#ST = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\")\n",
    "ST = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "query= \"hello \"\n",
    "embedded_query = ST.encode(query)\n",
    "embedded_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embedded_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "#from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings # new lib\n",
    "\n",
    "embed_model = \"nomic-embed-text\" # we can use same embed model as llama3.1 as they came in pairs\n",
    "embeddings = OllamaEmbeddings(model=embed_model)\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a retriever\n",
    "\n",
    "We can use a retriever to find chunks in the vector store that are similar to a supplied question.\n",
    "\n",
    "<img src='images/retriever1.png' width=\"1000\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'FAQ_GDG.pdf', 'page': 9}, page_content='GDSC\\nFAQ\\nThe\\npurpose\\nof\\nthis\\ndocument\\nis\\nto\\ncapture\\nfrequently\\nasked\\nquestions\\nabout\\nthe\\nGDSC\\nprogram.\\nJoin\\nGDSC\\nWho\\nshould\\njoin\\nGoogle\\nDeveloper\\nStudent\\nClubs?\\nCollege\\nand\\nuniv ersity\\nstudents\\nar e\\nencour aged\\nt o\\njoin\\nGoogle\\nDe v eloper\\nStudent\\nClubs.\\nCan\\nI\\njoin\\nmultiple\\nchapters?\\nY ou\\ncan\\npar ticipate\\nin\\ne v ents\\nor ganiz ed\\nb y\\nmultiple\\nchapters,\\nhowe v er\\nif\\ny ou\\ndecide\\nt o\\ndedicate\\ny ourself\\nt o\\nbecome\\na\\nGDSC\\nLead\\nor\\nCor e\\nT eam\\nMember ,\\ny ou\\nwill\\nbe\\noﬃcially\\nassigned\\nt o\\none\\nchapter .\\nWhat\\ndoes\\na\\nGDSC\\nlead\\ndo?\\nIn\\ngener al,\\nGDSC\\nleaders\\nar e\\nfocused\\non\\nthe\\nfollowing\\nar eas:\\n●\\nStar t\\na\\nclub\\n-\\nW ork\\nwith\\ny our\\nuniv ersity\\nor\\ncollege\\nt o\\nstar t\\na\\nstudent\\nclub.\\nSelect\\na\\ncor e\\nteam\\nand'),\n",
       " Document(metadata={'source': 'FAQ_GDG.pdf', 'page': 10}, page_content=\"Lead\\napplication\\n.\\n●\\nW e 'll\\nr e view\\ny our\\nsubmission\\nand\\nget\\nback\\nt o\\ny ou\\nvia\\nemail\\nas\\nsoon\\nas\\npossible.\\nBenefits\\nWhat\\nare\\nthe\\nbenefits\\nof\\nbecoming\\na\\nGDSC\\nlead?\"),\n",
       " Document(metadata={'source': 'FAQ_GDG.pdf', 'page': 11}, page_content='GDSC\\nLeads\\nshould\\nbe\\na v ailable\\nt o\\nrun\\nan\\ne v ent\\nideally\\nonce\\na\\nmonth,\\nand\\nat\\nleast\\ne v er y\\nthr ee\\nmonths\\nt o\\nr emain\\nan\\nactiv e\\nGDSC\\nchapter .\\nAdditionally ,\\nrunning\\na\\nGDSC\\nis\\na\\none\\ny ear\\ncommitment.\\nTimeline\\nWhat\\nis\\nthe\\ntimeline\\nfor\\napplying\\nfor\\nthe\\nGDSC\\nLead\\nposition?\\nW e\\naccept\\napplications\\nonce\\nper\\ny ear ,\\nbetween\\nApril\\nand\\nA ugust.\\nPlease\\nfollow\\nthis\\npage\\nfor\\nthe\\nnew\\ndeadlines\\nand\\nthe\\nGDSC\\ne v ents\\nplatform\\nt o\\ncheck\\ncurr ent\\nchapters.'),\n",
       " Document(metadata={'source': 'FAQ_GDG.pdf', 'page': 11}, page_content='Ther e\\nar e\\na\\nnumber\\nof\\nbeneﬁts\\nt o\\nleading\\na\\nGDSC\\nchapter ,\\nbut\\nher e\\nar e\\na\\nf ew\\nthat\\nstand\\nout:\\n●\\nPr of essional\\ngr owth\\n-\\nAccess\\nt o\\ncommunity\\nmanagement\\ntr aining\\nand\\ntechnical\\nknowledge\\nt o\\nhelp\\ny ou\\nbe\\na\\nstr onger\\nleader ,\\nand\\nr eceiv e\\ninvitations\\nt o\\nselect\\nGoogle\\ne v ents.\\n●\\nNetwork\\ngr owth\\n-\\nAccess\\nt o\\na\\nglobal\\nnetwork\\nof\\nstudent\\nleaders,\\npr of essional\\ncommunity\\nor ganiz ers,\\nindustr y\\nexper ts,\\nand\\nGooglers\\nt o\\ngain\\nment orship\\nand\\nshar e\\nknowledge.\\n●\\nCommunity\\nlearning\\n-\\nDedicated\\nsuppor t\\nt o\\nhelp\\neducate\\nand\\nexpand\\ny our\\ncommunity\\nonline\\nand\\nin-person.\\nWhat\\nis\\nthe\\ntime\\ncommitment?\\nGDSC\\nLeads\\nshould\\nbe\\na v ailable\\nt o\\nrun\\nan\\ne v ent\\nideally\\nonce\\na\\nmonth,\\nand\\nat\\nleast\\ne v er y')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "retriever.invoke(\"what is GDSC ?\")\n",
    "# 4 k- similar chunks by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the model\n",
    "\n",
    "We'll be using Ollama to load the local model in memory. After creating the model, we can invoke it with a question to get the response back.\n",
    "\n",
    "<img src='images/model1.png' width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', 'docstore', 'document_loaders', 'utils', 'vectorstores']\n"
     ]
    }
   ],
   "source": [
    "import langchain_community\n",
    "print(dir(langchain_community))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='As of my last update in April 2023, Joe Biden is the President of the United States. He took office on January 20, 2021, succeeding Donald Trump as the 46th President of the United States. Please note that this information might change over time due to elections or other political developments.', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2024-11-28T13:38:25.9847052Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 30240396900, 'load_duration': 17220478000, 'prompt_eval_count': 19, 'prompt_eval_duration': 2285017000, 'eval_count': 65, 'eval_duration': 10723552000}, id='run-e63e6834-903d-45cc-8a93-aa7bd0f88662-0', usage_metadata={'input_tokens': 19, 'output_tokens': 65, 'total_tokens': 84})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(model=MODEL, temperature=0)\n",
    "model.invoke(\"Who is the president of the United States?\") # eww whats is that \"AIMEssage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-groq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(\n",
    "    temperature=0,\n",
    "    model= \"llama-3.1-70b-versatile\",#MODEL\n",
    "    api_key=\"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\",\n",
    "    verbose= True,\n",
    "    max_retries=3,\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the model's response\n",
    "\n",
    "The response from the model is an `AIMessage` instance containing the answer. We can extract the text answer by using the appropriate output parser. We can connect the model and the parser using a chain.\n",
    "\n",
    "<img src='images/parser1.png' width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tunisia is a country located in North Africa. It is situated in the Maghreb region, bordered by:\n",
      "\n",
      "1. Algeria to the west\n",
      "2. Libya to the southeast\n",
      "3. Mediterranean Sea to the north and east\n",
      "\n",
      "Tunisia is a relatively small country, with a total area of approximately 163,610 square kilometers (63,170 square miles). Its capital and largest city is Tunis, which is located in the northeastern part of the country.\n",
      "\n",
      "Geographically, Tunisia is characterized by a diverse landscape, with mountains, deserts, and coastal plains. The country has a long coastline along the Mediterranean Sea, with several important ports, including the Port of Tunis and the Port of Sfax.\n",
      "\n",
      "Here's a rough outline of Tunisia's location:\n",
      "\n",
      "* Latitude: 30° - 38° N\n",
      "* Longitude: 7° - 12° E\n",
      "\n",
      "Tunisia is a strategic location, connecting Europe, Africa, and the Middle East, making it an important hub for trade, culture, and tourism.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = model | parser \n",
    "print(chain.invoke(\"where tunisia is located\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a prompt\n",
    "\n",
    "In addition to the question we want to ask, we also want to provide the model with the context from the PDF file. We can use a prompt template to define and reuse the prompt we'll use with the model.\n",
    "\n",
    "\n",
    "<img src='images/prompt1.png' width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an assistant that provides answers to questions based on\n",
      "a given context. \n",
      "\n",
      "Answer the question based on the context. If you can't answer the\n",
      "question, reply \"I don't know\".\n",
      "\n",
      "Be as concise as possible and go straight to the point.\n",
      "\n",
      "Context: Here is some context\n",
      "\n",
      "Question: Here is a question\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an assistant that provides answers to questions based on\n",
    "a given context. \n",
    "\n",
    "Answer the question based on the context. If you can't answer the\n",
    "question, reply \"I don't know\".\n",
    "\n",
    "Be as concise as possible and go straight to the point.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "print(prompt.format(context=\"Here is some context\", question=\"Here is a question\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the prompt to the chain\n",
    "\n",
    "We can now chain the prompt with the model and the parser.\n",
    "\n",
    "<img src='images/chain11.png' width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Anna.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\n",
    "    \"context\": \"Anna's sister is Susan\", \n",
    "    \"question\": \"Who is Susan's sister?\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the retriever to the chain\n",
    "\n",
    "Finally, we can connect the retriever to the chain to get the context from the vector store.\n",
    "\n",
    "context is list of 4 most similar\n",
    "\n",
    "<img src='images/chain22.png' width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the chain to answer questions\n",
    "\n",
    "Finally, we can use the chain to ask questions that will be answered using the PDF document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is GDG ?\n",
      "Answer: The Google Developer Group offers tools and resources to help you on your development journey.\n",
      "*************************\n",
      "\n",
      "Question: What is GDSC ?\n",
      "Answer: GDSC stands for Google Developer Student Clubs.\n",
      "*************************\n",
      "\n",
      "Question: What is GDE?\n",
      "Answer: GDE stands for Google Developer Experts.\n",
      "*************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What is GDG ?\",\n",
    "    \"What is GDSC ?\",\n",
    "    \"What is GDE?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {chain.invoke({'question': question})}\")\n",
    "    print(\"*************************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q= \"how many members in GDG carthage ? \"\n",
    "\n",
    "chain.invoke({'question': q})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To join GDG: Visit the members site at https://gdg.community.dev/ and join a chapter or apply to create a new chapter if none exists in your area.\\n\\nTo become a GDE: You need to have solid expertise in an area featuring Google technology, be passionate about giving back to the community, and meet the eligibility criteria.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q= \"how can i join GDG and be a GDE \"\n",
    "chain.invoke({'question': q})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No, there is no cost to join a chapter or attend events.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q= \"do i need to pay any fees to be a member in GDG and attend workshops ? \"\n",
    "\n",
    "chain.invoke({'question': q})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What to do next ? \n",
    "\n",
    "Play with the size of chunks\n",
    "\n",
    "Try different documents \n",
    "\n",
    "Try multiple documents not just one.\n",
    "\n",
    "Different model other than llama3.1 \n",
    "\n",
    "Try different embedding models.\n",
    "\n",
    "Try different vectorstore databases\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gn",
   "language": "python",
   "name": "gn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
