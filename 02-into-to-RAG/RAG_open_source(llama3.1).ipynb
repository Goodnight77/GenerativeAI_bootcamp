{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Gentle Introduction to RAG Applications\n",
    "\n",
    "This notebook creates a simple RAG (Retrieval-Augmented Generation) system to answer questions from a PDF document using an open-source model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI\\.cache\\huggingface\\hub\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "# Get the default cache directory\n",
    "from transformers import TRANSFORMERS_CACHE\n",
    "\n",
    "print(TRANSFORMERS_CACHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI/.cache\\torch\\hub\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check default directory where models are saved\n",
    "print(torch.hub.get_dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI/.cache/huggingface/transformers/\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "print(os.path.expanduser('~/.cache/huggingface/transformers/'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the path of the Hugging Face cache directory\n",
    "hf_cache_dir = os.path.expanduser('~/.cache/huggingface/transformers/')\n",
    "\n",
    "# List the contents of the cache directory\n",
    "for root, dirs, files in os.walk(hf_cache_dir):\n",
    "    print(\"Downloaded models:\", dirs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change dir \n",
    "os.environ['TRANSFORMERS_CACHE'] = '/new/path/to/store/models' #HF \n",
    "torch.hub.set_dir('/new/path/to/store/models') #pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_FILE = \"FAQ_GDG.pdf\"\n",
    "\n",
    "# We'll be using Llama 3.1 8B for this example.\n",
    "MODEL = \"llama3.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the PDF document\n",
    "\n",
    "Let's start by loading the PDF document and breaking it down into separate pages.\n",
    "\n",
    "<img src='images/documents1.png' width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages: 12\n",
      "Length of a page: 2000\n",
      "Content of a page: W h y\n",
      "w a s\n",
      "m y\n",
      "a c c o u n t\n",
      "t u r n e d\n",
      "t o\n",
      "p r i v a t e ?\n",
      "If\n",
      "we\n",
      "r easonably\n",
      "belie v e\n",
      "content\n",
      "in\n",
      "y our\n",
      "pr oﬁle\n",
      "violates\n",
      "our\n",
      "content\n",
      "policy\n",
      ",\n",
      "y our\n",
      "account\n",
      "will\n",
      "be\n",
      "switched\n",
      "t o\n",
      "priv ate\n",
      "and\n",
      "the\n",
      "content\n",
      "in\n",
      "y our\n",
      "pr oﬁle\n",
      "will\n",
      "be\n",
      "deleted.\n",
      "Y ou\n",
      "won 't\n",
      "be\n",
      "able\n",
      "t o\n",
      "mak e\n",
      "y our\n",
      "account\n",
      "public\n",
      "again\n",
      "for\n",
      "at\n",
      "least\n",
      "60\n",
      "da ys.\n",
      "Google\n",
      "also\n",
      "r eser v es\n",
      "the\n",
      "right\n",
      "t o\n",
      "suspend\n",
      "or\n",
      "terminate\n",
      "y our\n",
      "access\n",
      "t o\n",
      "the\n",
      "ser vices\n",
      "or\n",
      "delete\n",
      "y our\n",
      "Google\n",
      "Account,\n",
      "as\n",
      "described\n",
      "in\n",
      "the\n",
      "T aking\n",
      "action\n",
      "in\n",
      "case\n",
      "of\n",
      "pr oblems\n",
      "section\n",
      "of\n",
      "the\n",
      "Google\n",
      "T erms\n",
      "of\n",
      "Ser vice.\n",
      "W h a t\n",
      "h a p p e n s\n",
      "w h e n\n",
      "I\n",
      "i n t e g r a t e\n",
      "m y\n",
      "p r o ﬁ l e\n",
      "w i t h\n",
      "a\n",
      "t h i r d - p a r t y\n",
      "a p p\n",
      "o r\n",
      "s e r v i c e ?\n",
      "If\n",
      "y ou\n",
      "authoriz e\n",
      "an\n",
      "application\n",
      "t o\n",
      "access\n",
      "y our\n",
      "Google\n",
      "De v eloper\n",
      "Pr ogr am\n",
      "pr oﬁle,\n",
      "that\n",
      "application\n",
      "will\n",
      "be\n",
      "able\n",
      "t o\n",
      "see\n",
      "y our\n",
      "pr oﬁle\n",
      "information,\n",
      "e v en\n",
      "if\n",
      "y ou\n",
      "ha v e\n",
      "not\n",
      "made\n",
      "y our\n",
      "pr oﬁle\n",
      "public.\n",
      "Learn\n",
      "mor e\n",
      "about\n",
      "how\n",
      "t o\n",
      "manage\n",
      "thir d-par ty\n",
      "apps\n",
      "and\n",
      "ser vices\n",
      "with\n",
      "access\n",
      "t o\n",
      "y our\n",
      "account.\n",
      "C a n\n",
      "I\n",
      "t r a n s f e r\n",
      "o r\n",
      "m e r g e\n",
      "m y\n",
      "a c c o u n t\n",
      "w i t h\n",
      "a n o t h e r\n",
      "p r o ﬁ l e ?\n",
      "Y ou\n",
      "cannot\n",
      "tr ansf er\n",
      "or\n",
      "mer ge\n",
      "y our\n",
      "pr oﬁle\n",
      "with\n",
      "another\n",
      "account.\n",
      "It' s\n",
      "encour aged\n",
      "t o\n",
      "use\n",
      "y our\n",
      "personal\n",
      "account\n",
      "(when\n",
      "appr opriate)\n",
      "for\n",
      "y our\n",
      "Google\n",
      "De v eloper\n",
      "Pr ogr am\n",
      "pr oﬁle\n",
      "t o\n",
      "ensur e\n",
      "y ou\n",
      "r etain\n",
      "all\n",
      "the\n",
      "badges\n",
      "and\n",
      "information.\n",
      "W h y\n",
      "c a n ' t\n",
      "I\n",
      "c r e a t e\n",
      "a\n",
      "p r o ﬁ l e\n",
      "w i t h\n",
      "m y\n",
      "G o o g l e\n",
      "W o r k s p a c e\n",
      "a c c o u n t ?\n",
      "The\n",
      "Google\n",
      "De v eloper\n",
      "Pr ogr am\n",
      "pr oﬁle\n",
      "suppor ts\n",
      "Google\n",
      "W orkspace\n",
      "account\n",
      "types;\n",
      "howe v er ,\n",
      "if\n",
      "y ou\n",
      "ar e\n",
      "getting\n",
      "an\n",
      "err or\n",
      "y ou\n",
      "might\n",
      "need\n",
      "y our\n",
      "or ganization ' s\n",
      "administr at or\n",
      "t o\n",
      "enable\n",
      "access\n",
      "t o\n",
      "the\n",
      "Google\n",
      "De v elopers\n",
      "ser vice.\n",
      "F or\n",
      "mor e\n",
      "information,\n",
      "see\n",
      "T urn\n",
      "Google\n",
      "De v elopers\n",
      "on\n",
      "or\n",
      "off\n",
      "for\n",
      "users\n",
      ".\n",
      "W h e r e\n",
      "s h o u l d\n",
      "I\n",
      "ﬁ l e\n",
      "i s s u e s\n",
      "o r\n",
      "f e e d b a c k ?\n",
      "If\n",
      "y ou\n",
      "encounter\n",
      "any\n",
      "issues\n",
      "or\n",
      "want\n",
      "t o\n",
      "pr o vide\n",
      "f eedback\n",
      "on\n",
      "anything\n",
      "r elated\n",
      "t o\n",
      "y our\n",
      "Google\n",
      "De v eloper\n",
      "Pr ogr am\n",
      "pr oﬁle,\n",
      "click\n",
      "Send\n",
      "F eedback\n",
      "at\n",
      "the\n",
      "bott om\n",
      "of\n",
      "y our\n",
      "Google\n",
      "De v eloper\n",
      "Pr ogr am\n",
      "pr oﬁle\n",
      "page\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(PDF_FILE)\n",
    "pages = loader.load()\n",
    "\n",
    "print(f\"Number of pages: {len(pages)}\")\n",
    "print(f\"Length of a page: {len(pages[1].page_content)}\")\n",
    "print(\"Content of a page:\", pages[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the pages in chunks\n",
    "\n",
    "Pages are too long, so let's split pages into different chunks.\n",
    "\n",
    "<img src='images/splitter1.png' width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 33\n",
      "Length of a chunk: 693\n",
      "Content of a chunk: b y\n",
      "going\n",
      "t o\n",
      "de v elopers.google.com/pr oﬁle/u/me\n",
      ".\n",
      "H o w\n",
      "d o\n",
      "I\n",
      "e d i t\n",
      "m y\n",
      "p r o ﬁ l e ?\n",
      "Y ou\n",
      "can\n",
      "edit\n",
      "y our\n",
      "Google\n",
      "De v eloper\n",
      "Pr ogr am\n",
      "pr oﬁle\n",
      "b y\n",
      "going\n",
      "t o\n",
      "de v elopers.google.com/pr oﬁle/u/me\n",
      ".\n",
      "W h a t\n",
      "h a p p e n s\n",
      "i f\n",
      "I\n",
      "m a k e\n",
      "m y\n",
      "p r o ﬁ l e\n",
      "p u b l i c ?\n",
      "Making\n",
      "y our\n",
      "pr oﬁle\n",
      "public\n",
      "mak es\n",
      "it\n",
      "viewable\n",
      "b y\n",
      "any one\n",
      "online.\n",
      "This\n",
      "includes\n",
      "y our\n",
      "name,\n",
      "image,\n",
      "r ole,\n",
      "company\n",
      "or\n",
      "school,\n",
      "bio,\n",
      "badges\n",
      "y ou'v e\n",
      "r eceiv ed,\n",
      "stats,\n",
      "and\n",
      "y our\n",
      "social\n",
      "media\n",
      "links\n",
      "(including\n",
      "GitHub,\n",
      "GitLab,\n",
      "X,\n",
      "Link edIn,\n",
      "and\n",
      "Stack\n",
      "Ov erﬂow).\n",
      "Y our\n",
      "pages\n",
      "sa v ed,\n",
      "pages\n",
      "r ated,\n",
      "and\n",
      "e v ents\n",
      "attended\n",
      "ar e\n",
      "not\n",
      "par t\n",
      "of\n",
      "y our\n",
      "public\n",
      "pr oﬁle.\n",
      "Y ou\n",
      "can\n",
      "change\n",
      "y our\n",
      "pr oﬁle\n",
      "priv acy\n",
      "settings\n",
      "under\n",
      "the\n",
      "Account\n",
      "tab\n",
      "at\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=100)\n",
    "\n",
    "chunks = splitter.split_documents(pages)\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(f\"Length of a chunk: {len(chunks[1].page_content)}\")\n",
    "print(\"Content of a chunk:\", chunks[1].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing the chunks in a vector store\n",
    "\n",
    "We can now generate embeddings for every chunk and store them in a vector store.\n",
    "\n",
    "<img src='images/vectorstore1.png' width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain-community faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "print(faiss.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -qU langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embed_model = \"nomic-embed-text\"\n",
    "embeddings = OllamaEmbeddings(model=embed_model)\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a retriever\n",
    "\n",
    "We can use a retriever to find chunks in the vector store that are similar to a supplied question.\n",
    "\n",
    "<img src='images/retriever1.png' width=\"1000\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'FAQ_GDG.pdf', 'page': 9}, page_content='GDSC\\nFAQ\\nThe\\npurpose\\nof\\nthis\\ndocument\\nis\\nto\\ncapture\\nfrequently\\nasked\\nquestions\\nabout\\nthe\\nGDSC\\nprogram.\\nJoin\\nGDSC\\nWho\\nshould\\njoin\\nGoogle\\nDeveloper\\nStudent\\nClubs?\\nCollege\\nand\\nuniv ersity\\nstudents\\nar e\\nencour aged\\nt o\\njoin\\nGoogle\\nDe v eloper\\nStudent\\nClubs.\\nCan\\nI\\njoin\\nmultiple\\nchapters?\\nY ou\\ncan\\npar ticipate\\nin\\ne v ents\\nor ganiz ed\\nb y\\nmultiple\\nchapters,\\nhowe v er\\nif\\ny ou\\ndecide\\nt o\\ndedicate\\ny ourself\\nt o\\nbecome\\na\\nGDSC\\nLead\\nor\\nCor e\\nT eam\\nMember ,\\ny ou\\nwill\\nbe\\noﬃcially\\nassigned\\nt o\\none\\nchapter .\\nWhat\\ndoes\\na\\nGDSC\\nlead\\ndo?\\nIn\\ngener al,\\nGDSC\\nleaders\\nar e\\nfocused\\non\\nthe\\nfollowing\\nar eas:\\n●\\nStar t\\na\\nclub\\n-\\nW ork\\nwith\\ny our\\nuniv ersity\\nor\\ncollege\\nt o\\nstar t\\na\\nstudent\\nclub.\\nSelect\\na\\ncor e\\nteam\\nand'),\n",
       " Document(metadata={'source': 'FAQ_GDG.pdf', 'page': 11}, page_content='GDSC\\nLeads\\nshould\\nbe\\na v ailable\\nt o\\nrun\\nan\\ne v ent\\nideally\\nonce\\na\\nmonth,\\nand\\nat\\nleast\\ne v er y\\nthr ee\\nmonths\\nt o\\nr emain\\nan\\nactiv e\\nGDSC\\nchapter .\\nAdditionally ,\\nrunning\\na\\nGDSC\\nis\\na\\none\\ny ear\\ncommitment.\\nTimeline\\nWhat\\nis\\nthe\\ntimeline\\nfor\\napplying\\nfor\\nthe\\nGDSC\\nLead\\nposition?\\nW e\\naccept\\napplications\\nonce\\nper\\ny ear ,\\nbetween\\nApril\\nand\\nA ugust.\\nPlease\\nfollow\\nthis\\npage\\nfor\\nthe\\nnew\\ndeadlines\\nand\\nthe\\nGDSC\\ne v ents\\nplatform\\nt o\\ncheck\\ncurr ent\\nchapters.'),\n",
       " Document(metadata={'source': 'FAQ_GDG.pdf', 'page': 10}, page_content=\"Lead\\napplication\\n.\\n●\\nW e 'll\\nr e view\\ny our\\nsubmission\\nand\\nget\\nback\\nt o\\ny ou\\nvia\\nemail\\nas\\nsoon\\nas\\npossible.\\nBenefits\\nWhat\\nare\\nthe\\nbenefits\\nof\\nbecoming\\na\\nGDSC\\nlead?\"),\n",
       " Document(metadata={'source': 'FAQ_GDG.pdf', 'page': 11}, page_content='Ther e\\nar e\\na\\nnumber\\nof\\nbeneﬁts\\nt o\\nleading\\na\\nGDSC\\nchapter ,\\nbut\\nher e\\nar e\\na\\nf ew\\nthat\\nstand\\nout:\\n●\\nPr of essional\\ngr owth\\n-\\nAccess\\nt o\\ncommunity\\nmanagement\\ntr aining\\nand\\ntechnical\\nknowledge\\nt o\\nhelp\\ny ou\\nbe\\na\\nstr onger\\nleader ,\\nand\\nr eceiv e\\ninvitations\\nt o\\nselect\\nGoogle\\ne v ents.\\n●\\nNetwork\\ngr owth\\n-\\nAccess\\nt o\\na\\nglobal\\nnetwork\\nof\\nstudent\\nleaders,\\npr of essional\\ncommunity\\nor ganiz ers,\\nindustr y\\nexper ts,\\nand\\nGooglers\\nt o\\ngain\\nment orship\\nand\\nshar e\\nknowledge.\\n●\\nCommunity\\nlearning\\n-\\nDedicated\\nsuppor t\\nt o\\nhelp\\neducate\\nand\\nexpand\\ny our\\ncommunity\\nonline\\nand\\nin-person.\\nWhat\\nis\\nthe\\ntime\\ncommitment?\\nGDSC\\nLeads\\nshould\\nbe\\na v ailable\\nt o\\nrun\\nan\\ne v ent\\nideally\\nonce\\na\\nmonth,\\nand\\nat\\nleast\\ne v er y')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "retriever.invoke(\"what is GDSC ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the model\n",
    "\n",
    "We'll be using Ollama to load the local model in memory. After creating the model, we can invoke it with a question to get the response back.\n",
    "\n",
    "<img src='images/model.png' width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', 'docstore', 'document_loaders', 'embeddings', 'utils', 'vectorstores']\n"
     ]
    }
   ],
   "source": [
    "import langchain_community\n",
    "print(dir(langchain_community))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='As of my last update in April 2023, Joe Biden is the President of the United States. He took office on January 20, 2021, succeeding Donald Trump as the 46th President of the United States. Please note that this information might change over time due to elections or other political developments.', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2024-11-26T09:29:02.474455Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 45513527900, 'load_duration': 19073096000, 'prompt_eval_count': 19, 'prompt_eval_duration': 3279129000, 'eval_count': 65, 'eval_duration': 23143425000}, id='run-52f6821a-4be6-41ca-ae08-4127063e71fa-0', usage_metadata={'input_tokens': 19, 'output_tokens': 65, 'total_tokens': 84})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(model=MODEL, temperature=0)\n",
    "model.invoke(\"Who is the president of the United States?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-groq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(\n",
    "    temperature=0.4,\n",
    "    model= \"llama-3.1-70b-versatile\", #\"llama3-70b-8192\",\n",
    "    api_key=\"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\",\n",
    "    verbose= True,\n",
    "    max_retries=3,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the model's response\n",
    "\n",
    "The response from the model is an `AIMessage` instance containing the answer. We can extract the text answer by using the appropriate output parser. We can connect the model and the parser using a chain.\n",
    "\n",
    "<img src='images/parser.png' width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of my knowledge cutoff in 2023, the President of the United States is Joe Biden. However, please note that my information may not be up to date. For the most recent information, I recommend checking a reliable news source.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = model | parser \n",
    "print(chain.invoke(\"Who is the president of the United States?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a prompt\n",
    "\n",
    "In addition to the question we want to ask, we also want to provide the model with the context from the PDF file. We can use a prompt template to define and reuse the prompt we'll use with the model.\n",
    "\n",
    "\n",
    "<img src='images/prompt.png' width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an assistant that provides answers to questions based on\n",
      "a given context. \n",
      "\n",
      "Answer the question based on the context. If you can't answer the\n",
      "question, reply \"I don't know\".\n",
      "\n",
      "Be as concise as possible and go straight to the point.\n",
      "\n",
      "Context: Here is some context\n",
      "\n",
      "Question: Here is a question\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an assistant that provides answers to questions based on\n",
    "a given context. \n",
    "\n",
    "Answer the question based on the context. If you can't answer the\n",
    "question, reply \"I don't know\".\n",
    "\n",
    "Be as concise as possible and go straight to the point.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "print(prompt.format(context=\"Here is some context\", question=\"Here is a question\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the prompt to the chain\n",
    "\n",
    "We can now chain the prompt with the model and the parser.\n",
    "\n",
    "<img src='images/chain11.png' width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Anna.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\n",
    "    \"context\": \"Anna's sister is Susan\", \n",
    "    \"question\": \"Who is Susan's sister?\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the retriever to the chain\n",
    "\n",
    "Finally, we can connect the retriever to the chain to get the context from the vector store.\n",
    "\n",
    "<img src='images/chain22.png' width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the chain to answer questions\n",
    "\n",
    "Finally, we can use the chain to ask questions that will be answered using the PDF document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is GDG ?\n",
      "Answer: GDG stands for Google Developer Groups.\n",
      "*************************\n",
      "\n",
      "Question: What is GDSC ?\n",
      "Answer: The context doesn't explicitly define what GDSC is, but based on the content, it appears to be \"Google Developer Student Clubs\".\n",
      "*************************\n",
      "\n",
      "Question: What is GDE?\n",
      "Answer: GDE stands for Google Developer Experts.\n",
      "*************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What is GDG ?\",\n",
    "    \"What is GDSC ?\",\n",
    "    \"What is GDE?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {chain.invoke({'question': question})}\")\n",
    "    print(\"*************************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q= \"how many members in GDG carthage ? \"\n",
    "\n",
    "chain.invoke({'question': q})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To join GDG: \\n1. Visit the members site at https://gdg.community.dev/. \\n2. If a nearby chapter doesn’t exist, you can apply to create a new GDG chapter in your city.\\n\\nTo become a GDE (Google Developer Expert), you need to meet certain requirements (mentioned in the context), but the application process is not explicitly mentioned in the context.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q= \"how can i join GDG and be a GDE \"\n",
    "chain.invoke({'question': q})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No, there is no cost to join a chapter or attend events.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q= \"do i need to pay any fees to be a member in GDG and attend workshops ? \"\n",
    "\n",
    "chain.invoke({'question': q})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gn",
   "language": "python",
   "name": "gn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
