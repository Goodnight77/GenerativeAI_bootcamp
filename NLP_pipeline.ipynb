{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOWkyMEchy8rsQQ2oNkMjoP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["<a href=\"https://github.com/Goodnight77/GenerativeAI_bootcamp/blob/main/NLP_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"],"metadata":{"id":"dTtjTudurF6o"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3WXAtuevfOtF","executionInfo":{"status":"ok","timestamp":1728575122940,"user_tz":-60,"elapsed":292,"user":{"displayName":"Mohammed Arbi","userId":"01482638175499677709"}},"outputId":"8c198081-8c7f-4bdc-d9cb-389a9084c30e"},"outputs":[{"output_type":"stream","name":"stdout","text":["b'NLP workshop   ????'\n","b'\\xe8\\x87\\xaa\\xe7\\x84\\xb6\\xe8\\xa8\\x80\\xe8\\xaa\\x9e\\xe5\\x87\\xa6\\xe7\\x90\\x86\\xe3\\x83\\xaf\\xe3\\x83\\xbc\\xe3\\x82\\xaf\\xe3\\x82\\xb7\\xe3\\x83\\xa7\\xe3\\x83\\x83\\xe3\\x83\\x97 ????'\n"]}],"source":["\n","# Unicode Nomalization\n","text = \"NLP workshop   ????\"\n","print(text.encode('utf-8'))\n","\n","text1 = '自然言語処理ワークショップ ????'\n","print(text1.encode('utf-8'))"]},{"cell_type":"markdown","source":["###  Text Cleaning :"],"metadata":{"id":"Iy8SVPEbHGuK"}},{"cell_type":"code","source":["import re\n","text = \"\"\"<gdg>\n","#GDG is a community\n","url <https://www.gdgcarthage.org/>,\n","email <gdg.carthage@gmail.com>\n","\"\"\"\n","def clean_text(text):\n","    # remove HTML TAG\n","    html = re.compile('[<,#*?>]')\n","    text = html.sub(r'',text)\n","    # Remove urls:\n","    url = re.compile('https?://\\S+|www\\.S+')\n","    text = url.sub(r'',text)\n","    # Remove email id:\n","    email = re.compile('[A-Za-z0-2]+@[\\w]+.[\\w]+')\n","    text = email.sub(r'',text)\n","    return text\n","print(clean_text(text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9FDMS2jafRGS","executionInfo":{"status":"ok","timestamp":1728575123270,"user_tz":-60,"elapsed":4,"user":{"displayName":"Mohammed Arbi","userId":"01482638175499677709"}},"outputId":"e1da9aae-7a2e-423a-8029-1394e6cdc0b7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["gdg\n","GDG is a community\n","url \n","email gdg.\n","\n"]}]},{"cell_type":"markdown","source":["###  Text Preprocessing"],"metadata":{"id":"WD8vtXVcHJ1K"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-0RcU7f7ghwA","executionInfo":{"status":"ok","timestamp":1728575147180,"user_tz":-60,"elapsed":9037,"user":{"displayName":"Mohammed Arbi","userId":"01482638175499677709"}},"outputId":"e147e9ef-6095-43e7-adfb-936c07db7678"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/words.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["####"],"metadata":{"id":"hwLlhIS3mvnu"}},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import SnowballStemmer, WordNetLemmatizer\n","from nltk.tag import pos_tag\n","from nltk.chunk import ne_chunk\n","import string\n","\n","# sample text to be preprocessed\n","text = \"\"\"GDG Carthage is an independent group; our activities and the opinions expressed\n","          here should in no way be linked to Google,\n","          the corporation. To learn more about the GDG program,\n","          visit https://developers.google.com/community/gdg/\"\"\"\n","\n","# tokenize the text\n","tokens = word_tokenize(text)\n","\n","# remove stop words\n","stop_words = set(stopwords.words('english'))\n","filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n","\n","# perform stemming and lemmatization\n","stemmer = SnowballStemmer('english')\n","lemmatizer = WordNetLemmatizer()\n","stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n","lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n","\n","# remove digits and punctuation\n","cleaned_tokens = [token for token in lemmatized_tokens\n","\t\t\t\tif not token.isdigit() and not token in string.punctuation]\n","\n","# convert all tokens to lowercase\n","lowercase_tokens = [token.lower() for token in cleaned_tokens]\n","\n","# perform part-of-speech (POS) tagging\n","pos_tags = pos_tag(lowercase_tokens)\n","\n","# perform named entity recognition (NER)\n","named_entities = ne_chunk(pos_tags)\n","\n","# print the preprocessed text\n","print(\"Original text:\", text)\n","print(\"Preprocessed tokens:\", lowercase_tokens)\n","print(\"POS tags:\", pos_tags)\n","print(\"Named entities:\", named_entities)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AYjKy3e9fRRO","executionInfo":{"status":"ok","timestamp":1728575153716,"user_tz":-60,"elapsed":6539,"user":{"displayName":"Mohammed Arbi","userId":"01482638175499677709"}},"outputId":"2c5d3dd5-df10-4d9c-8042-90a92173a920"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original text: GDG Carthage is an independent group; our activities and the opinions expressed\n","          here should in no way be linked to Google,\n","          the corporation. To learn more about the GDG program,\n","          visit https://developers.google.com/community/gdg/\n","Preprocessed tokens: ['gdg', 'carthage', 'independent', 'group', 'activity', 'opinion', 'expressed', 'way', 'linked', 'google', 'corporation', 'learn', 'gdg', 'program', 'visit', 'http', '//developers.google.com/community/gdg/']\n","POS tags: [('gdg', 'JJ'), ('carthage', 'NN'), ('independent', 'JJ'), ('group', 'NN'), ('activity', 'NN'), ('opinion', 'NN'), ('expressed', 'VBD'), ('way', 'NN'), ('linked', 'VBN'), ('google', 'JJ'), ('corporation', 'NN'), ('learn', 'NN'), ('gdg', 'NN'), ('program', 'NN'), ('visit', 'NN'), ('http', 'NN'), ('//developers.google.com/community/gdg/', 'NN')]\n","Named entities: (S\n","  gdg/JJ\n","  carthage/NN\n","  independent/JJ\n","  group/NN\n","  activity/NN\n","  opinion/NN\n","  expressed/VBD\n","  way/NN\n","  linked/VBN\n","  google/JJ\n","  corporation/NN\n","  learn/NN\n","  gdg/NN\n","  program/NN\n","  visit/NN\n","  http/NN\n","  //developers.google.com/community/gdg//NN)\n"]}]},{"cell_type":"markdown","source":["### Feature *Engineering*"],"metadata":{"id":"SAKYjaIolFoc"}},{"cell_type":"markdown","source":["#### One Hot Encoder"],"metadata":{"id":"Ac7Fvm3AlIQo"}},{"cell_type":"code","source":["import nltk\n","# nltk.download('punkt') # Download 'punkt'\n","# from nltk if it's not downloaded\n","from nltk.tokenize import sent_tokenize\n","Text = \"\"\"Google developer group.\n","\t\tgoogle Learning Together.\n","\t\ttoday NLP workshop.\n","\t\tLearning NLP techniques\"\"\"\n","sentences = sent_tokenize(Text)\n","sentences = [sent.lower().replace(\".\", \"\") for sent in sentences]\n","print('Tokenized Sentences :', sentences)\n","\n","# Create the vocabulary\n","vocab = {}\n","count = 0\n","for sent in sentences:\n","\tfor word in sent.split():\n","\t\tif word not in vocab:\n","\t\t\tcount = count + 1\n","\t\t\tvocab[word] = count\n","print('vocabulary :', vocab)\n","\n","# One Hot Encoding\n","def OneHotEncoder(text):\n","\tonehot_encoded = []\n","\tfor word in text.split():\n","\t\ttemp = [0]*len(vocab)\n","\t\tif word in vocab:\n","\t\t\ttemp[vocab[word]-1] = 1\n","\t\t\tonehot_encoded.append(temp)\n","\treturn onehot_encoded\n","\n","\n","# print('\\n',sentences[0])\n","print('OneHotEncoded vector for sentence : \"',\n","\tsentences[0], '\"is \\n', OneHotEncoder(sentences[0]))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nThUy28rfRT1","executionInfo":{"status":"ok","timestamp":1728575251000,"user_tz":-60,"elapsed":264,"user":{"displayName":"Mohammed Arbi","userId":"01482638175499677709"}},"outputId":"a33f707f-217a-49bb-e567-577a8cdf6f13"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized Sentences : ['google developer group', 'google learning together', 'today nlp workshop', 'learning nlp techniques']\n","vocabulary : {'google': 1, 'developer': 2, 'group': 3, 'learning': 4, 'together': 5, 'today': 6, 'nlp': 7, 'workshop': 8, 'techniques': 9}\n","OneHotEncoded vector for sentence : \" google developer group \"is \n"," [[1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0]]\n"]}]},{"cell_type":"markdown","source":["#### Bag of Word(Bow)"],"metadata":{"id":"6MN4R9m_ldAK"}},{"cell_type":"code","source":["import nltk\n","#nltk.download('punkt') # Download 'punkt' from nltk if it's not downloaded\n","from nltk.tokenize import sent_tokenize\n","from sklearn.feature_extraction.text import CountVectorizer\n","Text = Text\n","# TOKENIZATION\n","sentences = sent_tokenize(Text)\n","sentences = [sent.lower().replace(\".\",\"\") for sent in sentences]\n","print('Our Corpus:',sentences)\n","#CountVectorizer : Convert a collection of text documents to a matrix of token counts.\n","count_vect = CountVectorizer()\n","# fit & transform will represent each sentences as BOW representation\n","BOW = count_vect.fit_transform(sentences)\n","# Get the vocabulary\n","print(\"Our vocabulary: \", count_vect.vocabulary_)\n","#see the BOW representation\n","print(f\"BoW representation for {sentences[0]} {BOW[0].toarray()}\")\n","print(f\"BoW representation for {sentences[1]} {BOW[1].toarray()}\")\n","print(f\"BoW representation for {sentences[2]} {BOW[2].toarray()}\")\n","# BOW representation for a new text\n","BOW_ = count_vect.transform([\"learning what is nlp \"])\n","print(\"Bow representation for 'learning what is nlp ':\", BOW_.toarray())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5054q9FCfRWP","executionInfo":{"status":"ok","timestamp":1728575254154,"user_tz":-60,"elapsed":301,"user":{"displayName":"Mohammed Arbi","userId":"01482638175499677709"}},"outputId":"69bad8ae-89e5-4213-95ad-368e5a70bd6b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Our Corpus: ['google developer group', 'google learning together', 'today nlp workshop', 'learning nlp techniques']\n","Our vocabulary:  {'google': 1, 'developer': 0, 'group': 2, 'learning': 3, 'together': 7, 'today': 6, 'nlp': 4, 'workshop': 8, 'techniques': 5}\n","BoW representation for google developer group [[1 1 1 0 0 0 0 0 0]]\n","BoW representation for google learning together [[0 1 0 1 0 0 0 1 0]]\n","BoW representation for today nlp workshop [[0 0 0 0 1 0 1 0 1]]\n","Bow representation for 'learning what is nlp ': [[0 0 0 1 1 0 0 0 0]]\n"]}]},{"cell_type":"markdown","source":["#### Bag of n-grams"],"metadata":{"id":"Kx3JAXJvlkTT"}},{"cell_type":"code","source":["import nltk\n","# nltk.download('punkt') # Download 'punkt'\n","# from nltk if it's not downloaded\n","from nltk.tokenize import sent_tokenize\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","Text = \"\"\"Google developer group.\n","\t\tgoogle Learning Together.\n","\t\ttoday NLP workshop.\n","\t\tLearning NLP techniques\"\"\"\n","\n","# TOKENIZATION\n","sentences = sent_tokenize(Text)\n","sentences = [sent.lower().replace(\".\", \"\") for sent in sentences]\n","print('Our Corpus:', sentences)\n","\n","# Ngram vectorization example with count\n","# vectorizer and uni, bi, trigrams\n","count_vect = CountVectorizer(ngram_range=(1, 3))\n","\n","# fit & transform will represent each sentences\n","# as Bag of n-grams representation\n","BOW_nGram = count_vect.fit_transform(sentences)\n","\n","# Get the vocabulary\n","print(\"Our vocabulary:\\n\", count_vect.vocabulary_)\n","\n","# see the Bag of n-grams representation\n","print('Ngram representation for \"{}\" is {}'\n","\t.format(sentences[0], BOW_nGram[0].toarray()))\n","print('Ngram representation for \"{}\" is {}'\n","\t.format(sentences[1], BOW_nGram[1].toarray()))\n","print('Ngram representation for \"{}\" is {}'.\n","\tformat(sentences[2], BOW_nGram[2].toarray()))\n","\n","# Bag of n-grams representation for a new text\n","BOW_nGram_ = count_vect.transform([\"learning dsa from geeksforgeeks together\"])\n","print(\"Ngram representation for 'learning dsa from geeksforgeeks together' is\",\n","\tBOW_nGram_.toarray())\n"],"metadata":{"id":"YOFHbA6ffRY_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728575256996,"user_tz":-60,"elapsed":312,"user":{"displayName":"Mohammed Arbi","userId":"01482638175499677709"}},"outputId":"08fc909e-3569-4210-aa45-73eaacd55e99"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Our Corpus: ['google developer group', 'google learning together', 'today nlp workshop', 'learning nlp techniques']\n","Our vocabulary:\n"," {'google': 2, 'developer': 0, 'group': 7, 'google developer': 3, 'developer group': 1, 'google developer group': 4, 'learning': 8, 'together': 19, 'google learning': 5, 'learning together': 11, 'google learning together': 6, 'today': 16, 'nlp': 12, 'workshop': 20, 'today nlp': 17, 'nlp workshop': 14, 'today nlp workshop': 18, 'techniques': 15, 'learning nlp': 9, 'nlp techniques': 13, 'learning nlp techniques': 10}\n","Ngram representation for \"google developer group\" is [[1 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n","Ngram representation for \"google learning together\" is [[0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0]]\n","Ngram representation for \"today nlp workshop\" is [[0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 1]]\n","Ngram representation for 'learning dsa from geeksforgeeks together' is [[0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0]]\n"]}]},{"cell_type":"markdown","source":["#### TF-IDF"],"metadata":{"id":"x9MTtEEtlpbV"}},{"cell_type":"code","source":["import nltk\n","# nltk.download('punkt') # Download 'punkt'\n","# from nltk if it's not downloaded\n","from nltk.tokenize import sent_tokenize\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","Text = \"\"\"Google developer group.\n","\t\tgoogle Learning Together.\n","\t\ttoday NLP workshop.\n","\t\tLearning NLP techniques\"\"\"\n","\n","# TOKENIZATION\n","sentences = sent_tokenize(Text)\n","sentences = [sent.lower().replace(\".\", \"\") for sent in sentences]\n","print('Our Corpus:', sentences)\n","\n","# TF-IDF\n","tfidf = TfidfVectorizer()\n","tfidf_matrix = tfidf.fit_transform(sentences)\n","\n","# All words in the vocabulary.\n","print(\"vocabulary\", tfidf.get_feature_names_out())\n","# IDF value for all words in the vocabulary\n","print(\"IDF for all words in the vocabulary :\\n\", tfidf.idf_)\n","\n","# TFIDF representation for all documents in our corpus\n","print('\\nTFIDF representation for \"{}\" is \\n{}'\n","\t.format(sentences[0], tfidf_matrix[0].toarray()))\n","print('TFIDF representation for \"{}\" is \\n{}'\n","\t.format(sentences[1], tfidf_matrix[1].toarray()))\n","print('TFIDF representation for \"{}\" is \\n{}'\n","\t.format(sentences[2],tfidf_matrix[2].toarray()))\n","\n","# TFIDF representation for a new text\n","matrix = tfidf.transform([\"learning dsa from geeksforgeeks\"])\n","print(\"\\nTFIDF representation for 'learning dsa from geeksforgeeks' is\\n\",\n","\tmatrix.toarray())\n"],"metadata":{"id":"6av4MLc7fRbf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728575291191,"user_tz":-60,"elapsed":289,"user":{"displayName":"Mohammed Arbi","userId":"01482638175499677709"}},"outputId":"2fe82b79-db3e-465b-d0b6-99e816266db1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Our Corpus: ['google developer group', 'google learning together', 'today nlp workshop', 'learning nlp techniques']\n","vocabulary ['developer' 'google' 'group' 'learning' 'nlp' 'techniques' 'today'\n"," 'together' 'workshop']\n","IDF for all words in the vocabulary :\n"," [1.91629073 1.51082562 1.91629073 1.51082562 1.51082562 1.91629073\n"," 1.91629073 1.91629073 1.91629073]\n","\n","TFIDF representation for \"google developer group\" is \n","[[0.61761437 0.48693426 0.61761437 0.         0.         0.\n","  0.         0.         0.        ]]\n","TFIDF representation for \"google learning together\" is \n","[[0.         0.52640543 0.         0.52640543 0.         0.\n","  0.         0.66767854 0.        ]]\n","TFIDF representation for \"today nlp workshop\" is \n","[[0.         0.         0.         0.         0.48693426 0.\n","  0.61761437 0.         0.61761437]]\n","\n","TFIDF representation for 'learning dsa from geeksforgeeks' is\n"," [[0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"]}]},{"cell_type":"markdown","source":["#### Neural Approach (Word embedding)"],"metadata":{"id":"b-ICAlLylthR"}},{"cell_type":"code","source":[],"metadata":{"id":"Eqz-XMBgfReG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vUfZyEq7fRg1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Pre-Trained Word Embeddings"],"metadata":{"id":"Pdt-kD5wlz_y"}},{"cell_type":"markdown","source":["##### Word2vec by Google"],"metadata":{"id":"OgnrYyRJGdxR"}},{"cell_type":"code","source":["import gensim.downloader as api\n","\n","# load the pre-trained Word2Vec model\n","model = api.load('word2vec-google-news-300')\n","\n","# define word pairs to compute similarity for\n","word_pairs = [('learn', 'learning'), ('india', 'indian'), ('fame', 'famous')]\n","\n","# compute similarity for each pair of words\n","for pair in word_pairs:\n","\tsimilarity = model.similarity(pair[0], pair[1])\n","\tprint(f\"Similarity between '{pair[0]}' and '{pair[1]}' using Word2Vec: {similarity:.3f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8aOheDmaGVxP","executionInfo":{"status":"ok","timestamp":1728575876170,"user_tz":-60,"elapsed":547736,"user":{"displayName":"Mohammed Arbi","userId":"01482638175499677709"}},"outputId":"816a642b-31fa-4bd0-9c86-cde6bc22e123"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[==================================================] 100.0% 1662.8/1662.8MB downloaded\n","Similarity between 'learn' and 'learning' using Word2Vec: 0.637\n","Similarity between 'india' and 'indian' using Word2Vec: 0.697\n","Similarity between 'fame' and 'famous' using Word2Vec: 0.326\n"]}]},{"cell_type":"markdown","source":["##### GloVe by Stanford"],"metadata":{"id":"kpLCN95kGgnU"}},{"cell_type":"code","source":["!pip uninstall torchtext -y\n","!pip install torchdata --extra-index-url https://download.pytorch.org/whl/cu118\n","!pip install torchtext -q"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oqgZTF6UKL2T","executionInfo":{"status":"ok","timestamp":1728576055123,"user_tz":-60,"elapsed":9320,"user":{"displayName":"Mohammed Arbi","userId":"01482638175499677709"}},"outputId":"cb6555d4-4b9f-4e66-952a-765af147741c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: torchtext 0.18.0\n","Uninstalling torchtext-0.18.0:\n","  Successfully uninstalled torchtext-0.18.0\n","Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118\n","Collecting torchdata\n","  Downloading torchdata-0.8.0-cp310-cp310-manylinux1_x86_64.whl.metadata (5.4 kB)\n","Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.2.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.32.3)\n","Requirement already satisfied: torch>=2 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.4.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2024.6.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (2024.8.30)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2->torchdata) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2->torchdata) (1.3.0)\n","Downloading torchdata-0.8.0-cp310-cp310-manylinux1_x86_64.whl (2.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: torchdata\n","Successfully installed torchdata-0.8.0\n"]}]},{"cell_type":"code","source":["import torch\n","import torchtext.vocab as vocab\n","\n","# load the pre-trained GloVe model\n","glove = vocab.GloVe(name='840B', dim=300)\n","\n","# define word pairs to compute similarity for\n","word_pairs = [('learn', 'learning'), ('india', 'indian'), ('fame', 'famous')]\n","\n","# compute similarity for each pair of words\n","for pair in word_pairs:\n","\tvec1, vec2 = glove[pair[0]], glove[pair[1]]\n","\tsimilarity = torch.dot(vec1, vec2) / (torch.norm(vec1) * torch.norm(vec2))\n","\tprint(f\"Similarity between '{pair[0]}' and '{pair[1]}' using GloVe: {similarity:.3f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"id":"AYqu82TlGlIr","executionInfo":{"status":"error","timestamp":1728576055124,"user_tz":-60,"elapsed":8,"user":{"displayName":"Mohammed Arbi","userId":"01482638175499677709"}},"outputId":"e8702234-9f86-4ff8-b25f-2bc475b34e2f"},"execution_count":null,"outputs":[{"output_type":"error","ename":"OSError","evalue":"/usr/local/lib/python3.10/dist-packages/torchtext/lib/libtorchtext.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-0bb9e9636c79>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# load the pre-trained GloVe model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mglove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGloVe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'840B'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchtext/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# the following import has to happen first in order to load the torchtext C++ library\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_extension\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0m_TEXT_BUCKET\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://download.pytorch.org/models/text/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchtext/_extension.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0m_init_extension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchtext/_extension.py\u001b[0m in \u001b[0;36m_init_extension\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torchtext C++ Extension is not found.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0m_load_lib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"libtorchtext\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;31m# This import is for initializing the methods registered via PyBind11\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# This has to happen after the base library is loaded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchtext/_extension.py\u001b[0m in \u001b[0;36m_load_lib\u001b[0;34m(lib)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_library\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36mload_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1293\u001b[0m             \u001b[0;31m# static (global) initialization code in order to register custom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m             \u001b[0;31m# operators with the JIT.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m             \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloaded_libraries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: /usr/local/lib/python3.10/dist-packages/torchtext/lib/libtorchtext.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs"]}]},{"cell_type":"markdown","source":["##### fasttext by Facebook"],"metadata":{"id":"89PEdfTtGmkL"}},{"cell_type":"code","source":["import gensim.downloader as api\n","\n","# load the pre-trained fastText model\n","fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\")\n","\n","# define word pairs to compute similarity for\n","word_pairs = [('learn', 'learning'), ('india', 'indian'), ('fame', 'famous')]\n","\n","# compute similarity for each pair of words\n","for pair in word_pairs:\n","\tsimilarity = fasttext_model.similarity(pair[0], pair[1])\n","\tprint(f\"Similarity between '{pair[0]}' and '{pair[1]}' using Word2Vec: {similarity:.3f}\")\n"],"metadata":{"id":"4210egbpGmTs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cSHy6ClYl5yx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"UU7mr_CPfRjP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"h2E1OXERfRl3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"lBco1l4hfRoj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FyWIIPRffRr7"},"execution_count":null,"outputs":[]}]}