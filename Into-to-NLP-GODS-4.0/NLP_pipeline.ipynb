{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOyNvSJ29T6KZQJIME5Mwix"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["<a href=\"https://github.com/Goodnight77/GenerativeAI_bootcamp/blob/main/NLP_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"],"metadata":{"id":"dTtjTudurF6o"}},{"cell_type":"markdown","source":["###  Text Cleaning :"],"metadata":{"id":"Iy8SVPEbHGuK"}},{"cell_type":"code","source":["import re\n","text = \"\"\"<gdg>\n","#GDG is a community\n","url <https://www.gdgcarthage.org/>,\n","email <gdg.carthage@gmail.com>\n","\"\"\"\n","def clean_text(text):\n","    # remove HTML TAG\n","    html = re.compile('[<,#*?>]')\n","    text = html.sub(r'',text)\n","    # Remove urls:\n","    url = re.compile('https?://\\S+|www\\.S+')\n","    text = url.sub(r'',text)\n","    # Remove email id:\n","    email = re.compile('[A-Za-z0-2]+@[\\w]+.[\\w]+')\n","    text = email.sub(r'',text)\n","    return text\n","print(clean_text(text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9FDMS2jafRGS","executionInfo":{"status":"ok","timestamp":1728575123270,"user_tz":-60,"elapsed":4,"user":{"displayName":"Mohammed Arbi","userId":"01482638175499677709"}},"outputId":"e1da9aae-7a2e-423a-8029-1394e6cdc0b7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["gdg\n","GDG is a community\n","url \n","email gdg.\n","\n"]}]},{"cell_type":"markdown","source":["###  Text Preprocessing"],"metadata":{"id":"WD8vtXVcHJ1K"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","nltk.download('punkt_tab')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')\n","nltk.download('averaged_perceptron_tagger_eng')\n","nltk.download('maxent_ne_chunker_tab')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-0RcU7f7ghwA","executionInfo":{"status":"ok","timestamp":1738592781902,"user_tz":-60,"elapsed":553,"user":{"displayName":"Mohammed Arbi","userId":"01482638175499677709"}},"outputId":"4cd4a409-5c52-41e1-a780-a8886a2d28ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package maxent_ne_chunker_tab to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["#### NER & POS"],"metadata":{"id":"hwLlhIS3mvnu"}},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import SnowballStemmer, WordNetLemmatizer\n","from nltk.tag import pos_tag\n","from nltk.chunk import ne_chunk\n","import string\n","\n","# sample text to be preprocessed\n","text = \"\"\"GDG is an independent group; our activities and the opinions expressed\n","          here should in no way be linked to Google,\n","          the corporation. To learn more about the GDG program,\n","          visit https://developers.google.com/community/gdg/\"\"\"\n","\n","# tokenize the text\n","tokens = word_tokenize(text)\n","\n","# remove stop words\n","stop_words = set(stopwords.words('english'))\n","filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n","\n","# perform stemming and lemmatization\n","stemmer = SnowballStemmer('english')\n","lemmatizer = WordNetLemmatizer()\n","stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n","lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n","\n","# remove digits and punctuation\n","cleaned_tokens = [token for token in lemmatized_tokens\n","\t\t\t\tif not token.isdigit() and not token in string.punctuation]\n","\n","# convert all tokens to lowercase\n","lowercase_tokens = [token.lower() for token in cleaned_tokens]\n","\n","# perform part-of-speech (POS) tagging\n","pos_tags = pos_tag(lowercase_tokens)\n","\n","# perform named entity recognition (NER)\n","named_entities = ne_chunk(pos_tags)\n","\n","# print the preprocessed text\n","print(\"Original text:\", text)\n","print(\"Preprocessed tokens:\", lowercase_tokens)\n","print(\"POS tags:\", pos_tags)\n","print(\"Named entities:\", named_entities)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AYjKy3e9fRRO","executionInfo":{"status":"ok","timestamp":1738592785045,"user_tz":-60,"elapsed":408,"user":{"displayName":"Mohammed Arbi","userId":"01482638175499677709"}},"outputId":"cf1975d6-3746-4b52-e6c8-d88e7d6953bd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original text: GDG is an independent group; our activities and the opinions expressed\n","          here should in no way be linked to Google,\n","          the corporation. To learn more about the GDG program,\n","          visit https://developers.google.com/community/gdg/\n","Preprocessed tokens: ['gdg', 'independent', 'group', 'activity', 'opinion', 'expressed', 'way', 'linked', 'google', 'corporation', 'learn', 'gdg', 'program', 'visit', 'http', '//developers.google.com/community/gdg/']\n","POS tags: [('gdg', 'JJ'), ('independent', 'JJ'), ('group', 'NN'), ('activity', 'NN'), ('opinion', 'NN'), ('expressed', 'VBD'), ('way', 'NN'), ('linked', 'VBN'), ('google', 'JJ'), ('corporation', 'NN'), ('learn', 'NN'), ('gdg', 'NN'), ('program', 'NN'), ('visit', 'NN'), ('http', 'NN'), ('//developers.google.com/community/gdg/', 'NN')]\n","Named entities: (S\n","  gdg/JJ\n","  independent/JJ\n","  group/NN\n","  activity/NN\n","  opinion/NN\n","  expressed/VBD\n","  way/NN\n","  linked/VBN\n","  google/JJ\n","  corporation/NN\n","  learn/NN\n","  gdg/NN\n","  program/NN\n","  visit/NN\n","  http/NN\n","  //developers.google.com/community/gdg//NN)\n"]}]},{"cell_type":"markdown","source":["### Feature *Engineering*"],"metadata":{"id":"SAKYjaIolFoc"}},{"cell_type":"markdown","source":["#### One Hot Encoder"],"metadata":{"id":"Ac7Fvm3AlIQo"}},{"cell_type":"code","source":["import nltk\n","# nltk.download('punkt') # Download 'punkt'\n","# from nltk if it's not downloaded\n","from nltk.tokenize import sent_tokenize\n","Text = \"\"\"Go data science hackathon.\n","\t\tNLP Learning Together.\n","\t\ttoday NLP workshop.\n","\t\tLearning NLP techniques\"\"\"\n","sentences = sent_tokenize(Text)\n","sentences = [sent.lower().replace(\".\", \"\") for sent in sentences]\n","print('Tokenized Sentences :', sentences)\n","\n","# Create the vocabulary\n","vocab = {}\n","count = 0\n","for sent in sentences:\n","\tfor word in sent.split():\n","\t\tif word not in vocab:\n","\t\t\tcount = count + 1\n","\t\t\tvocab[word] = count\n","print('vocabulary :', vocab)\n","\n","# One Hot Encoding\n","def OneHotEncoder(text):\n","\tonehot_encoded = []\n","\tfor word in text.split():\n","\t\ttemp = [0]*len(vocab)\n","\t\tif word in vocab:\n","\t\t\ttemp[vocab[word]-1] = 1\n","\t\t\tonehot_encoded.append(temp)\n","\treturn onehot_encoded\n","\n","\n","# print('\\n',sentences[0])\n","print('OneHotEncoded vector for sentence : \"',\n","\tsentences[0], '\"is \\n', OneHotEncoder(sentences[0]))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nThUy28rfRT1","executionInfo":{"status":"ok","timestamp":1738592730383,"user_tz":-60,"elapsed":269,"user":{"displayName":"Mohammed Arbi","userId":"01482638175499677709"}},"outputId":"d1ea5f19-9960-4a57-e18d-b0bf837d3ba3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized Sentences : ['go data science hackathon', 'nlp learning together', 'today nlp workshop', 'learning nlp techniques']\n","vocabulary : {'go': 1, 'data': 2, 'science': 3, 'hackathon': 4, 'nlp': 5, 'learning': 6, 'together': 7, 'today': 8, 'workshop': 9, 'techniques': 10}\n","OneHotEncoded vector for sentence : \" go data science hackathon \"is \n"," [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]\n"]}]},{"cell_type":"markdown","source":["#### Bag of Word(Bow)\n","\n","1. Words in vocabulary: ['google', 'developer', 'group']\n","\n","* developer → Index 0 → 1 occurrence\n","* google → Index 1 → 1 occurrence\n","* group → Index 2 → 1 occurrence\n","that's why we have [1 1 1 0 0 0 0 0 0]\n","also,\n","2. Words in vocabulary: ['google', 'learning', 'together']\n","\n","* google → Index 1 → 1 occurrence\n","* learning → Index 3 → 1 occurrence\n","* together → Index 7 → 1 occurrence\n","* All other words are absent (0s).\n","\n","\n"],"metadata":{"id":"6MN4R9m_ldAK"}},{"cell_type":"code","source":["import nltk\n","#nltk.download('punkt') # Download 'punkt' from nltk if it's not downloaded\n","from nltk.tokenize import sent_tokenize\n","from sklearn.feature_extraction.text import CountVectorizer\n","Text = Text\n","# TOKENIZATION\n","sentences = sent_tokenize(Text)\n","sentences = [sent.lower().replace(\".\",\"\") for sent in sentences]\n","print('Our Corpus:',sentences)\n","#CountVectorizer : Convert a collection of text documents to a matrix of token counts.\n","count_vect = CountVectorizer()\n","# fit & transform will represent each sentences as BOW representation\n","BOW = count_vect.fit_transform(sentences)\n","# Get the vocabulary\n","print(\"Our vocabulary: \", count_vect.vocabulary_)\n","#see the BOW representation\n","print(f\"BoW representation for {sentences[0]} {BOW[0].toarray()}\")\n","print(f\"BoW representation for {sentences[1]} {BOW[1].toarray()}\")\n","print(f\"BoW representation for {sentences[2]} {BOW[2].toarray()}\")\n","# BOW representation for a new text\n","BOW_ = count_vect.transform([\"learning what is nlp \"])\n","print(\"Bow representation for 'learning what is nlp ':\", BOW_.toarray())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5054q9FCfRWP","executionInfo":{"status":"ok","timestamp":1728575254154,"user_tz":-60,"elapsed":301,"user":{"displayName":"Mohammed Arbi","userId":"01482638175499677709"}},"outputId":"69bad8ae-89e5-4213-95ad-368e5a70bd6b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Our Corpus: ['google developer group', 'google learning together', 'today nlp workshop', 'learning nlp techniques']\n","Our vocabulary:  {'google': 1, 'developer': 0, 'group': 2, 'learning': 3, 'together': 7, 'today': 6, 'nlp': 4, 'workshop': 8, 'techniques': 5}\n","BoW representation for google developer group [[1 1 1 0 0 0 0 0 0]]\n","BoW representation for google learning together [[0 1 0 1 0 0 0 1 0]]\n","BoW representation for today nlp workshop [[0 0 0 0 1 0 1 0 1]]\n","Bow representation for 'learning what is nlp ': [[0 0 0 1 1 0 0 0 0]]\n"]}]},{"cell_type":"markdown","source":["#### Bag of n-grams\n","\n","Your vocabulary includes:\n","\n","1. Unigrams (single words): 'go', 'data', 'science', 'hackathon', etc.\n","2. Bigrams (two-word phrases): 'go data', 'data science', etc.\n","3. Trigrams (three-word phrases): 'go data science', 'data science hackathon', etc.\n","Each n-gram is indexed in the vocabulary.\n","\n","* example: \"go data science hackathon\"\n","[1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0]\n","* \"go\" → ✅ (1)\n","* \"data\" → ✅ (1)\n","* \"science\" → ✅ (1)\n","* \"hackathon\" → ✅ (1)\n","* \"go data\" → ✅ (1)\n","* \"data science\" → ✅ (1)\n","* \"science hackathon\" → ✅ (1)\n","* \"go data science\" → ❌ (0)\n","* \"data science hackathon\" → ❌ (0)\n","\n","* Usage : Text classification, topic modeling, search engines\n","* Purpose : Capture word sequences"],"metadata":{"id":"Kx3JAXJvlkTT"}},{"cell_type":"code","source":["import nltk\n","# nltk.download('punkt') # Download 'punkt'\n","# from nltk if it's not downloaded\n","from nltk.tokenize import sent_tokenize\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","Text = \"\"\"Go data science hackathon.\n","\t\tNLP Learning Together.\n","\t\ttoday NLP workshop.\n","\t\tLearning NLP techniques\"\"\"\n","\n","# TOKENIZATION\n","sentences = sent_tokenize(Text)\n","sentences = [sent.lower().replace(\".\", \"\") for sent in sentences]\n","print('Our Corpus:', sentences)\n","\n","# Ngram vectorization example with count\n","# vectorizer and uni, bi, trigrams\n","count_vect = CountVectorizer(ngram_range=(1, 3))\n","\n","# fit & transform will represent each sentences\n","# as Bag of n-grams representation\n","BOW_nGram = count_vect.fit_transform(sentences)\n","\n","# Get the vocabulary\n","print(\"Our vocabulary:\\n\", count_vect.vocabulary_)\n","\n","# see the Bag of n-grams representation\n","print('Ngram representation for \"{}\" is {}'\n","\t.format(sentences[0], BOW_nGram[0].toarray()))\n","print('Ngram representation for \"{}\" is {}'\n","\t.format(sentences[1], BOW_nGram[1].toarray()))\n","print('Ngram representation for \"{}\" is {}'.\n","\tformat(sentences[2], BOW_nGram[2].toarray()))\n","\n","# Bag of n-grams representation for a new text\n","BOW_nGram_ = count_vect.transform([\"learning NLP text preprocessing from this workshop\"])\n","print(\"Ngram representation for 'learning NLP text preprocessing from this workshop' is\",\n","\tBOW_nGram_.toarray())\n"],"metadata":{"id":"YOFHbA6ffRY_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738592723248,"user_tz":-60,"elapsed":346,"user":{"displayName":"Mohammed Arbi","userId":"01482638175499677709"}},"outputId":"967fb59d-6e4a-43c8-d524-ae5c62e7fa19"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Our Corpus: ['go data science hackathon', 'nlp learning together', 'today nlp workshop', 'learning nlp techniques']\n","Our vocabulary:\n"," {'go': 3, 'data': 0, 'science': 16, 'hackathon': 6, 'go data': 4, 'data science': 1, 'science hackathon': 17, 'go data science': 5, 'data science hackathon': 2, 'nlp': 11, 'learning': 7, 'together': 22, 'nlp learning': 12, 'learning together': 10, 'nlp learning together': 13, 'today': 19, 'workshop': 23, 'today nlp': 20, 'nlp workshop': 15, 'today nlp workshop': 21, 'techniques': 18, 'learning nlp': 8, 'nlp techniques': 14, 'learning nlp techniques': 9}\n","Ngram representation for \"go data science hackathon\" is [[1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0]]\n","Ngram representation for \"nlp learning together\" is [[0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0]]\n","Ngram representation for \"today nlp workshop\" is [[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 1]]\n","Ngram representation for 'learning NLP text preprocessing from this workshop' is [[0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1]]\n"]}]},{"cell_type":"markdown","source":["#### TF-IDF"],"metadata":{"id":"x9MTtEEtlpbV"}},{"cell_type":"code","source":["import nltk\n","# nltk.download('punkt') # Download 'punkt'\n","# from nltk if it's not downloaded\n","from nltk.tokenize import sent_tokenize\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","Text = \"\"\"Go data science hackathon.\n","\t\tNLP Learning Together.\n","\t\ttoday NLP workshop.\n","\t\tLearning NLP techniques\"\"\"\n","\n","# TOKENIZATION\n","sentences = sent_tokenize(Text)\n","sentences = [sent.lower().replace(\".\", \"\") for sent in sentences]\n","print('Our Corpus:', sentences)\n","\n","# TF-IDF\n","tfidf = TfidfVectorizer()\n","tfidf_matrix = tfidf.fit_transform(sentences)\n","\n","# All words in the vocabulary.\n","print(\"vocabulary\", tfidf.get_feature_names_out())\n","# IDF value for all words in the vocabulary\n","print(\"IDF for all words in the vocabulary :\\n\", tfidf.idf_)\n","\n","# TFIDF representation for all documents in our corpus\n","print('\\nTFIDF representation for \"{}\" is \\n{}'\n","\t.format(sentences[0], tfidf_matrix[0].toarray()))\n","print('TFIDF representation for \"{}\" is \\n{}'\n","\t.format(sentences[1], tfidf_matrix[1].toarray()))\n","print('TFIDF representation for \"{}\" is \\n{}'\n","\t.format(sentences[2],tfidf_matrix[2].toarray()))\n","\n","# TFIDF representation for a new text\n","matrix = tfidf.transform([\"learning NLP text preprocessing from this workshop\"])\n","print(\"\\nTFIDF representation for 'learning NLP text preprocessing from this workshop' is\\n\",\n","\tmatrix.toarray())\n"],"metadata":{"id":"6av4MLc7fRbf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738592698095,"user_tz":-60,"elapsed":306,"user":{"displayName":"Mohammed Arbi","userId":"01482638175499677709"}},"outputId":"fd8c3828-be80-4aef-c03f-e38ec995c09b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Our Corpus: ['go data science hackathon', 'nlp learning together', 'today nlp workshop', 'learning nlp techniques']\n","vocabulary ['data' 'go' 'hackathon' 'learning' 'nlp' 'science' 'techniques' 'today'\n"," 'together' 'workshop']\n","IDF for all words in the vocabulary :\n"," [1.91629073 1.91629073 1.91629073 1.51082562 1.22314355 1.91629073\n"," 1.91629073 1.91629073 1.91629073 1.91629073]\n","\n","TFIDF representation for \"go data science hackathon\" is \n","[[0.5 0.5 0.5 0.  0.  0.5 0.  0.  0.  0. ]]\n","TFIDF representation for \"nlp learning together\" is \n","[[0.         0.         0.         0.55349232 0.44809973 0.\n","  0.         0.         0.70203482 0.        ]]\n","TFIDF representation for \"today nlp workshop\" is \n","[[0.         0.         0.         0.         0.41137791 0.\n","  0.         0.64450299 0.         0.64450299]]\n","\n","TFIDF representation for 'learning NLP text preprocessing from this workshop' is\n"," [[0.         0.         0.         0.55349232 0.44809973 0.\n","  0.         0.         0.         0.70203482]]\n"]}]},{"cell_type":"markdown","source":["### Pre-Trained Word Embeddings"],"metadata":{"id":"Pdt-kD5wlz_y"}},{"cell_type":"markdown","source":["##### Word2vec by Google"],"metadata":{"id":"OgnrYyRJGdxR"}},{"cell_type":"code","source":["import gensim.downloader as api\n","\n","# load the pre-trained Word2Vec model\n","model = api.load('word2vec-google-news-300')\n","\n","# define word pairs to compute similarity for\n","word_pairs = [('learn', 'learning'), ('india', 'indian'), ('fame', 'famous')]\n","\n","# compute similarity for each pair of words\n","for pair in word_pairs:\n","\tsimilarity = model.similarity(pair[0], pair[1])\n","\tprint(f\"Similarity between '{pair[0]}' and '{pair[1]}' using Word2Vec: {similarity:.3f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8aOheDmaGVxP","executionInfo":{"status":"ok","timestamp":1728575876170,"user_tz":-60,"elapsed":547736,"user":{"displayName":"Mohammed Arbi","userId":"01482638175499677709"}},"outputId":"816a642b-31fa-4bd0-9c86-cde6bc22e123"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[==================================================] 100.0% 1662.8/1662.8MB downloaded\n","Similarity between 'learn' and 'learning' using Word2Vec: 0.637\n","Similarity between 'india' and 'indian' using Word2Vec: 0.697\n","Similarity between 'fame' and 'famous' using Word2Vec: 0.326\n"]}]},{"cell_type":"markdown","source":["##### GloVe by Stanford"],"metadata":{"id":"kpLCN95kGgnU"}},{"cell_type":"code","source":["!pip uninstall torchtext -y\n","!pip install torchdata --extra-index-url https://download.pytorch.org/whl/cu118 -q\n","!pip install torchtext --no-cache-dir -q"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oqgZTF6UKL2T","executionInfo":{"status":"ok","timestamp":1738592311517,"user_tz":-60,"elapsed":37654,"user":{"displayName":"Mohammed Arbi","userId":"01482638175499677709"}},"outputId":"68cd5e38-ea23-4bf6-d750-bb5e4984e566"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchdata-0.10.1\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["import torch\n","import torchtext.vocab as vocab\n","\n","# load the pre-trained GloVe model\n","glove = vocab.GloVe(name='840B', dim=300)\n","\n","# define word pairs to compute similarity for\n","word_pairs = [('learn', 'learning'), ('india', 'indian'), ('fame', 'famous')]\n","\n","# compute similarity for each pair of words\n","for pair in word_pairs:\n","\tvec1, vec2 = glove[pair[0]], glove[pair[1]]\n","\tsimilarity = torch.dot(vec1, vec2) / (torch.norm(vec1) * torch.norm(vec2))\n","\tprint(f\"Similarity between '{pair[0]}' and '{pair[1]}' using GloVe: {similarity:.3f}\")\n"],"metadata":{"id":"AYqu82TlGlIr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### fasttext by Facebook\n","![image.png](data:image/png;base64,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)"],"metadata":{"id":"89PEdfTtGmkL"}},{"cell_type":"code","source":["import gensim.downloader as api\n","\n","# load the pre-trained fastText model\n","fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\")\n","\n","# define word pairs to compute similarity for\n","word_pairs = [('learn', 'learning'), ('india', 'indian'), ('fame', 'famous')]\n","\n","# compute similarity for each pair of words\n","for pair in word_pairs:\n","\tsimilarity = fasttext_model.similarity(pair[0], pair[1])\n","\tprint(f\"Similarity between '{pair[0]}' and '{pair[1]}' using Word2Vec: {similarity:.3f}\")\n"],"metadata":{"id":"4210egbpGmTs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cSHy6ClYl5yx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"UU7mr_CPfRjP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"h2E1OXERfRl3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"lBco1l4hfRoj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FyWIIPRffRr7"},"execution_count":null,"outputs":[]}]}